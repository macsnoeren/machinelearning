{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Artificial Neural Network and Linear Algebra\n",
    "_Author: Maurice Snoeren_<br>\n",
    "Within this notebook, the artificial network is discussed and how this could be easily calculated by using linear algebra. Furthermore, the implementation of the equations are shown in Python.\n",
    "<img src=\"./images/ann1.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of an artificial neural network is shown by the figure above. This neural network has two layers. The input layer does not count, while this layer connects the input to the neural network. This neural network contains an input layer with three inputs, a hidden layer with four nodes and a output layer with two nodes. Deep neural networks contain more hidden layers. With more hidden layers the problem to solve can be more complex. The hidden layer performs the actual pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example contains six perceptrons and it is fully connected. That means that all the neurons are connected with each other between two layers. To calculate the activation of hidden layer nodes $h_1, h_2, h_3, h_4$, the following equations can be used:\n",
    "\n",
    "$z_{h1} = x_1*w_{11} + x_2*w_{21} + x_3*w_{31} + b_{h1}$<br>\n",
    "$h_1  = f(z_{h1})$\n",
    "\n",
    "$z_{h2} = x_1*w_{12} + x_2*w_{22} + x_3*w_{32} + b_{h2}$<br>\n",
    "$h_2  = f(z_{h2})$\n",
    "\n",
    "$z_{h3} = x_1*w_{13} + x_2*w_{23} + x_3*w_{33} + b_{h3}$<br>\n",
    "$h_3  = f(z_{h3})$\n",
    "\n",
    "$z_{h4} = x_1*w_{14} + x_2*w_{24} + x_3*w_{34} + b_{h4}$<br>\n",
    "$h_4  = f(z_{h4})$\n",
    "\n",
    "The output neurons $y_1, y_2$, taking the output of the hidden layer as input, can be calculated by\n",
    "\n",
    "$z_{y1} = h_1*w_{11} + h_2*w_{21} + h_3*w_{31} + h_4*w_{41} + b_{y1}$<br>\n",
    "$y_1  = f(z_{y1})$\n",
    "\n",
    "$z_{y2} = h_1*w_{12} + h_2*w_{22} + h_3*w_{32} + h_4*w_{42} + b_{y2}$<br>\n",
    "$y_2  = f(zh_{y2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot equations ... we cannot make it easier ... but by using linear algebra we are able to use vectors and matrices to reduce the mathematical equations. Starting with linear algebra we will show how a vector and a matrix are multiplied. \n",
    "\n",
    "<img src=\"./images/matrices1.png\" width=\"600px\" />\n",
    "\n",
    "If we change the number by variables that look a lot like the variables that are used by the neural network above, we see that linear algebra is suitable to represent the weights and nodes with respectevliy matrices and vectors. The figure below shows the calculation when a vector $x$ is multiplied (dot-multiplication) with matrix $w$. It results into an output vector $h$.\n",
    "\n",
    "<img src=\"./images/matrices2.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the theory of linear algebra we are able to rewrite the equation from above to vectors and matrices. The hidden layer vector $h$ and the output vector $y$ can be calculated by\n",
    "\n",
    "$z_h = x * W_{hx} + b_h$<br>\n",
    "$h   = f(z_h)$\n",
    "\n",
    "$z_y = h * W_{yh} + b_y$<br>\n",
    "$y   = f(z_y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simplifies the equations a lot. But ... note that still all calculation is done ... only the mathematical notation has been changed. Note that these equations are known as __forward propagation__. It calculates the output of the neural network from left to right or from the input to the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python example\n",
    "Within Python matrices and vectors can be used immediatly due to the use of numpy. That means that all equations can be taken over within Python. Within this example we will use the activation function sigmoid. The linear algebra equations of above can be implement as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output y: [0.55304507 0.55304507]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x): # Sigmoid activation function\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x   = np.array([0.6, 0.4, 0.3]) # inputs\n",
    "\n",
    "Whx = np.array([[0.1, 0.1, 0.1, 0.1], # weights from input x to hidden layer h\n",
    "                [0.1, 0.1, 0.1, 0.1],\n",
    "                [0.1, 0.1, 0.1, 0.1]])\n",
    "b_h = np.array([0, 0, 0, 0]) # bias vector of the hidden layer (4 nodes)\n",
    "\n",
    "Wyh = np.array([[0.1, 0.1], # weights from hidden layer h to output layer y\n",
    "                [0.1, 0.1],\n",
    "                [0.1, 0.1],\n",
    "                [0.1, 0.1]])\n",
    "b_y = np.array([0, 0]) # bias vector of the output layer y (2 nodes)\n",
    "\n",
    "# Forward propagation\n",
    "z_h = np.dot(x, Whx) + b_h\n",
    "h   = sigmoid(z_h)\n",
    "\n",
    "z_y = np.dot(h, Wyh) + b_y\n",
    "y   = sigmoid(z_y)\n",
    "                \n",
    "print(\"output y: \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
