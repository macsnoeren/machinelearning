{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Python implementation ANN from scratch back propagation\n",
    "_Author: Maurice Snoeren_<br>\n",
    "This notebook implement an ANN from scract in python with visualization. It will be used to gain extra insights of the weights and biases and how the optimizer is optimizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, num_input_nodes, num_output_nodes, output_activation): # construct the ANN object\n",
    "        self.num_input_nodes   = num_input_nodes   # Hold the number of input nodes\n",
    "        self.num_output_nodes  = num_output_nodes  # Hold the number of output nodes\n",
    "        self.output_activation = output_activation # Hold the number of input nodes\n",
    "        self.hidden_layers     = [] # Hold all the hidden_layer classes\n",
    "        self.Wy                = np.random.rand(num_input_nodes, num_output_nodes) # Hold output layer weight matrix\n",
    "        self.by                = np.zeros((1, num_output_nodes)) # Biases vector of the output nodes\n",
    "        self.x                 = [] # Hold the input vector that is used for calculation\n",
    "        self.zy                = [] # Hold the summation of the input and bias with the weights\n",
    "        self.y                 = [] # hold the output vector\n",
    "\n",
    "    def get_weight_matrix(self): # getter for the weight matrix\n",
    "        return self.Wy\n",
    "\n",
    "    def set_weight_matrix(self, Wy): # setter for the weight matrix\n",
    "        self.Wy = Wy\n",
    "\n",
    "    def get_biases_vector(self): # getter for the bias vector\n",
    "        return self.by\n",
    "\n",
    "    def set_biases_vector(self, by): # setter for the bias vector\n",
    "        self.by = by\n",
    "\n",
    "    def add_hidden_layer(self, hidden_layer): # add a new hidden layer to the ANN\n",
    "        self.hidden_layers.append(hidden_layer) # Add the HiddenLayer class to the array\n",
    "        self.Wy = np.random.rand(hidden_layer.num_hidden_nodes, self.num_output_nodes) # Re-initializes the output matrix\n",
    "                                                                                       # based on number of hidden nodes\n",
    "    def get_total_hidden_layers(self): # return how many hidden layers are configured\n",
    "        return len(self.hidden_layers)\n",
    "\n",
    "    def get_hidden_layer(self, i): # returns the hidden layer given the index (no checks performed!)\n",
    "        return self.hidden_layers[i]\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        self.x = x # store the input that we have used for the calculation\n",
    "\n",
    "        if ( len(self.hidden_layers) == 0): # Within our design it is possible that no hidden layers exist!\n",
    "            self.zy = np.dot( self.x, self.Wy ) + self.by\n",
    "            self.y = self.output_activation.forward( self.zy )\n",
    "\n",
    "        else: # when we have hidden layers, we iterate over these hidden layers\n",
    "            input_vector = self.x # this input_vector is used to pass to the next layer\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                output_vector = hidden_layer.forward_propagation(input_vector) # the hidden layer class calculates the \n",
    "                                                                               # output based on the input.\n",
    "                input_vector = output_vector # the next hidden layer will use the output of this hidden layer\n",
    "            \n",
    "            # calculate the output of the neural network using the output of the last hidden layer as input\n",
    "            self.zy = np.dot( input_vector, self.Wy ) + self.by # first calculate the weight and bias result\n",
    "            self.y  = self.output_activation.forward( self.zy ) # calculate the activation function\n",
    "\n",
    "        return self.y # return the output activation vector of all the nodes\n",
    "    \n",
    "    def cost_function(self, input_example, output_desired):\n",
    "        self.forward_propagation(input_example)  # Perform first the forward propagation calculation\n",
    "        J = 0.5 * ( self.y - output_desired )**2 # Calculate the cost function\n",
    "        return J\n",
    "\n",
    "    def back_propagation(self, input_example, output_desired):\n",
    "        J = self.cost_function(input_example, output_desired)\n",
    "        \n",
    "        # start back propagation of the network\n",
    "        dJ_dy = ( self.y - output_desired )\n",
    "        dy_dzy   = self.output_activation.derivative( self.zy )\n",
    "        \n",
    "        delta = np.multiply( dJ_dy, dy_dzy ) # required to back propagate through the network (part of the derivation that propagates back into the network)\n",
    "        weights = self.Wy # weight required for the next layer\n",
    "        dJ_dWh = [] # Hold the weight gradients of the hidden layers\n",
    "        dJ_dbh = [] # Hold the weight gradients of the biases\n",
    "\n",
    "        if ( len(self.hidden_layers) == 0): # When we do not have any hidden layers, we only have one weight matrix\n",
    "            dJ_dWy = np.dot( self.x.transpose(), delta )\n",
    "\n",
    "        else: # Loop over the hidden layers from back to start\n",
    "            dzy_dWy = self.hidden_layers[ len(self.hidden_layers)-1 ].h\n",
    "            dJ_dWy = np.dot( dzy_dWy.transpose(), delta )  # calculate the gradient of Wy\n",
    "            dJ_dby = delta\n",
    "            \n",
    "            for hidden_layer in reversed(self.hidden_layers): # loop the hidden layers from back to start (reversed!)\n",
    "                result  = hidden_layer.back_propagation(delta, weights) # calculate gradient of hidden layer Wh\n",
    "                delta   = result['delta'] # update delat for the next layer\n",
    "                weights = result['W'] # update the weights matrix for the next layer\n",
    "                dJ_dWh.append( result['dJ_dWh']) # append the gradient hidden weight matrix to the array\n",
    "                dJ_dbh.append( result['dJ_dbh'])\n",
    "\n",
    "        return {'dJ_dWh': list(reversed(dJ_dWh)), 'dJ_dWy': dJ_dWy,\n",
    "                'dJ_dbh': list(reversed(dJ_dbh)),'dJ_dby': dJ_dby } # return the back propagation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNHiddenLayer:\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, activation):\n",
    "        self.num_input_nodes  = num_input_nodes # number of nodes of the previous layer used as input\n",
    "        self.num_hidden_nodes = num_hidden_nodes # number of hidden nodes of this layer to be used\n",
    "        self.activation       = activation # the activation function that should be used for all hidden nodes\n",
    "        self.x                = [] # Input vector of this hidden layer\n",
    "        self.Wh               = np.random.rand(num_input_nodes, num_hidden_nodes) # Hidden weight matrix\n",
    "        self.bh               = np.zeros((1, num_hidden_nodes)) # Biases vector of the hidden layer\n",
    "        self.zh               = [] # Hold the summation of the input and bias with the weights\n",
    "        self.h                = [] # Hold the output vector of this hidden layer\n",
    "\n",
    "    def get_weight_matrix(self): # getter for the weight matrix of the hidden layer\n",
    "        return self.Wh\n",
    "\n",
    "    def set_weight_matrix(self, Wh): # setter for the weight matrix of the hidden layer\n",
    "        self.Wh = Wh\n",
    "\n",
    "    def get_biases_vector(self): # getter for the biases vector of the hidden layer\n",
    "        return self.bh\n",
    "\n",
    "    def set_biases_vector(self, bh): # setter for the biases vector of the hidden layer\n",
    "        self.bh = bh\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        self.x = x # store the input that is used for the calculation\n",
    "        self.zh = np.dot(x, self.Wh) + self.bh # first calculate the weight and bias result\n",
    "        self.h  = self.activation.forward( self.zh ) # calculate the activation function\n",
    "        \n",
    "        return self.h # return the output activation vector of all the nodes\n",
    "    \n",
    "    def back_propagation(self, prev_delta, prev_W):\n",
    "        delta = np.dot( prev_delta, prev_W.transpose() ) * self.activation.derivative( self.zh ) # this is required for\n",
    "                                                                                                 # next layer\n",
    "        dz1_dWh = self.x # Activation from the previous layer!\n",
    "        dJ_dWh  = np.dot( dz1_dWh.transpose(), delta ) # calculate the gradient of the weight matrix \n",
    "        dJ_dbh  = delta\n",
    "\n",
    "        return { 'delta': delta, 'W': self.Wh, 'dJ_dWh': dJ_dWh, 'dJ_dbh': dJ_dbh } # return the result of the backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNSigmoidActivation:\n",
    "    def forward(self, input_vector):\n",
    "        return 1/(1 + np.exp(-input_vector))\n",
    "    \n",
    "    def derivative(self, input_vector):\n",
    "        return np.exp(-input_vector) / ((1 + np.exp(-input_vector))**2)\n",
    "\n",
    "class ANNReLUActivation:\n",
    "    def forward(self, input_vector):\n",
    "        return np.maximum(0, input_vector)\n",
    "    \n",
    "    def derivative(self, input_vector):\n",
    "        temp = 1*input_vector # copy the data!\n",
    "        temp[temp>0]  = 1\n",
    "        temp[temp<=0] = 0\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is in place and we can now try the class and see whether the calculation is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#!pip install pygame\n",
    "import pygame\n",
    "\n",
    "pygame.init()  \n",
    "scr = pygame.display.set_mode((600,500))  \n",
    "pygame.display.set_caption('Pygame Window')\n",
    "done = False  \n",
    "while not done:  \n",
    "    for event in pygame.event.get():  \n",
    "        if event.type == pygame.QUIT:  \n",
    "            done = True  \n",
    "pygame.display.flip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [[5.62306227 4.88301714]]\n"
     ]
    }
   ],
   "source": [
    "#sa  = ANNSigmoidActivation() # construct the Sigmoid activation function\n",
    "sa  = ANNReLUActivation() # construct the ReLU activation function\n",
    "\n",
    "ann = ANN(4, 2, sa) # create an ANN with four input nodes and two output nodes. The output nodes get the sigmoid\n",
    "                    # activation function.\n",
    "    \n",
    "ann.add_hidden_layer(ANNHiddenLayer(4, 10, sa)) # add a hidden layer with ten nodes, the input is four due to the\n",
    "                                                  # total of number of input nodes x, defines by the ANN.\n",
    "    \n",
    "ann.add_hidden_layer(ANNHiddenLayer(10, 10, sa)) # create another hidden layer with ten node, the input is ten nodes\n",
    "                                                   # due to the input nodes of the previoud hidden layer of ten.\n",
    "\n",
    "x = np.array([[0.1, 0.1, 0.1, 0.1]]) # create an example input vector x\n",
    "\n",
    "print( \"output: \" + str(ann.forward_propagation(x)) ) # print the output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result: [[1.00000000e+00 4.71844785e-16]]\n",
      "Costs       : 8.388002686789728e-30\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 1, 1, 1]]) # input sample\n",
    "y = np.array([[1, 0]])       # desired output\n",
    "alpha = 0.0001                # learning rate\n",
    "\n",
    "for i in range(10000):\n",
    "    result = ann.back_propagation(x, y) # perform back propagation - 1 epoch met 1 data sample\n",
    "    \n",
    "    # update the weight matrices\n",
    "    ann.set_weight_matrix(ann.get_weight_matrix() - alpha*result['dJ_dWy']) # update output weight matrix Wy\n",
    "    ann.set_biases_vector(ann.get_biases_vector() - alpha*result['dJ_dby']) # update output weight matrix Wy\n",
    "    for i in range(ann.get_total_hidden_layers()): # update weight matrices of the hidden layers\n",
    "        hl = ann.get_hidden_layer(i)\n",
    "        wm = result['dJ_dWh'][i]\n",
    "        bm = result['dJ_dbh'][i]\n",
    "        hl.set_weight_matrix( hl.get_weight_matrix() - alpha*wm )\n",
    "        hl.set_biases_vector( hl.get_biases_vector() - alpha*bm )\n",
    "        \n",
    "result   = ann.forward_propagation(x)\n",
    "cost = np.mean(ann.cost_function(x, y))\n",
    "\n",
    "print( \"Final result: \" + str(result) )\n",
    "print( \"Costs       : \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7ed3ef7558cc172bb3e76ff7547d88cec8d49584a9128ac9174bb1bc741a054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
